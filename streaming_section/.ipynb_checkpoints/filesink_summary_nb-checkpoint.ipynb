{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2563081-c3a6-4992-b783-9163bb9f83b0",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Example\n",
    "\n",
    "Purpose: Reads a stream of messages from a Kafka topic and writes a stream of aggregations over sliding event-time window to memory.\n",
    "\n",
    "References: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "Author:  Gary A. Stafford\n",
    "\n",
    "Date: 2022-12-16\n",
    "\n",
    "Updated by Vasilios Anagnostopoulos 30/09/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b6543e-fc4c-4faf-9434-ee25cfcebd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    "    TimestampType,\n",
    "    BooleanType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322a21d0-4fb0-492e-b72d-327025bece2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_WATERMARK = \"10 seconds\"\n",
    "WINDOW_DURATION = \"20 seconds\"\n",
    "WINDOW_SLIDE = \"10 seconds\"\n",
    "PROCESSING_TIME = \"20 seconds\"\n",
    "SHOW_REFRESH = 20\n",
    "\n",
    "BOOTSTRAP_SERVERS = \"kafka:29092\"\n",
    "TOPIC_PURCHASES = \"demo.purchases\"\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": BOOTSTRAP_SERVERS,\n",
    "    \"subscribe\": TOPIC_PURCHASES,\n",
    "    \"startingOffsets\": \"latest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ecf312-4356-41ec-b816-c9f4702e7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.remote(\"sc://127.0.0.1:15002\").appName(\"kafka-streaming-query\").getOrCreate()\n",
    "df_sales = spark.readStream.format(\"kafka\").options(**KAFKA_OPTIONS).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f439c1a4-600a-41a3-aca4-cf125990c658",
   "metadata": {},
   "outputs": [
    {
     "ename": "RetriesExceeded",
     "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_MultiThreadedRendezvous\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\reattach.py:162\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._has_next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28mself\u001b[39m._current = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\reattach.py:281\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._call_iter\u001b[39m\u001b[34m(self, iter_fun)\u001b[39m\n\u001b[32m    280\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    283\u001b[39m     \u001b[38;5;66;03m# Remove the iterator, so that a new one will be created after retry.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\reattach.py:261\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._call_iter\u001b[39m\u001b[34m(self, iter_fun)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miter_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\reattach.py:163\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._has_next.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m._current = \u001b[38;5;28mself\u001b[39m._call_iter(\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\grpc\\_channel.py:543\u001b[39m, in \u001b[36m_Rendezvous.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\grpc\\_channel.py:955\u001b[39m, in \u001b[36m_MultiThreadedRendezvous._next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_response_ready\u001b[39m():\n",
      "\u001b[31m_MultiThreadedRendezvous\u001b[39m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:15002: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\r\n -- 10061)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:15002: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\\r\\n -- 10061)\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRetriesExceeded\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m      1\u001b[39m schema = StructType(\n\u001b[32m      2\u001b[39m     [\n\u001b[32m      3\u001b[39m         StructField(\u001b[33m\"\u001b[39m\u001b[33mtransaction_time\u001b[39m\u001b[33m\"\u001b[39m, TimestampType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     ]\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m ds_sales = (\n\u001b[32m     16\u001b[39m     df_sales.selectExpr(\u001b[33m\"\u001b[39m\u001b[33mCAST(value AS STRING)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     .select(F.from_json(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, schema=schema).alias(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     .start()\n\u001b[32m     39\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mds_sales\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\streaming\\query.py:97\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     95\u001b[39m await_termination_cmd = pb2.StreamingQueryCommand.AwaitTerminationCommand()\n\u001b[32m     96\u001b[39m cmd.await_termination.CopyFrom(await_termination_cmd)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_streaming_query_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\streaming\\query.py:185\u001b[39m, in \u001b[36mStreamingQuery._execute_streaming_query_cmd\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    183\u001b[39m exec_cmd = pb2.Command()\n\u001b[32m    184\u001b[39m exec_cmd.streaming_query_command.CopyFrom(cmd)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m (_, properties, _) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(pb2.StreamingQueryCommandResult, properties[\u001b[33m\"\u001b[39m\u001b[33mstreaming_query_command_result\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1148\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations)\u001b[39m\n\u001b[32m   1146\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1147\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m data, _, metrics, observed_metrics, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m   1152\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1560\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, self_destruct)\u001b[39m\n\u001b[32m   1557\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1537\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1812\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1810\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;28mself\u001b[39m._handle_rpc_error(error)\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1523\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_reattachable_execute:\n\u001b[32m   1519\u001b[39m     \u001b[38;5;66;03m# Don't use retryHandler - own retry handling is inside.\u001b[39;00m\n\u001b[32m   1520\u001b[39m     generator = ExecutePlanResponseReattachableIterator(\n\u001b[32m   1521\u001b[39m         req, \u001b[38;5;28mself\u001b[39m._stub, \u001b[38;5;28mself\u001b[39m._retrying, \u001b[38;5;28mself\u001b[39m._builder.metadata()\n\u001b[32m   1522\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhandle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.11-windows-x86_64-none\\Lib\\_collections_abc.py:356\u001b[39m, in \u001b[36mGenerator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    353\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the next item from the generator.\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[33;03m    When exhausted, raise StopIteration.\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\reattach.py:138\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator.send\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Any) -> pb2.ExecutePlanResponse:\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# will trigger reattach in case the stream completed without result_complete\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_has_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[32m    141\u001b[39m     ret = \u001b[38;5;28mself\u001b[39m._current\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\reattach.py:190\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._has_next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mself\u001b[39m._release_all()\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\reattach.py:158\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator._has_next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrying\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_current\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\retries.py:251\u001b[39m, in \u001b[36mRetrying.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._done:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\work\\spark-kafka-article\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\retries.py:236\u001b[39m, in \u001b[36mRetrying._wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Exceeded retries\u001b[39;00m\n\u001b[32m    235\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGiven up on retrying. error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(exception)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m RetriesExceeded(errorClass=\u001b[33m\"\u001b[39m\u001b[33mRETRIES_EXCEEDED\u001b[39m\u001b[33m\"\u001b[39m, messageParameters={}) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexception\u001b[39;00m\n",
      "\u001b[31mRetriesExceeded\u001b[39m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"transaction_time\", TimestampType(), False),\n",
    "        StructField(\"transaction_id\", StringType(), False),\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"price\", FloatType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"is_member\", BooleanType(), True),\n",
    "        StructField(\"member_discount\", FloatType(), True),\n",
    "        StructField(\"add_supplements\", BooleanType(), True),\n",
    "        StructField(\"supplement_price\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_sales = (\n",
    "    df_sales.selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(F.from_json(\"value\", schema=schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    "        .withColumn(\"sales\", F.col(\"price\")*F.col(\"quantity\"))\n",
    "    .withWatermark(\"transaction_time\", WINDOW_WATERMARK)\n",
    "    .groupBy(\"product_id\", F.window(\"transaction_time\", WINDOW_DURATION, WINDOW_SLIDE))\n",
    "    .agg(F.sum(\"sales\"), F.sum(\"quantity\"))\n",
    "    .select(\n",
    "        \"product_id\",\n",
    "        F.format_number(\"sum(sales)\", 2).alias(\"total_sales\"),\n",
    "        F.format_number(\"sum(quantity)\", 0).alias(\"total_items\"),\n",
    "        F.format_number(F.col(\"sum(sales)\")/F.col(\"sum(quantity)\"), 2).alias(\"mean_price\"),\n",
    "        \"window.start\",\n",
    "        \"window.end\",\n",
    "    )\n",
    "    .coalesce(1)\n",
    "    .writeStream\n",
    "    .trigger(processingTime=PROCESSING_TIME)\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"json\")\n",
    "    .option(\"path\", \"sliding_windows/someoutput\")\n",
    "    .option(\"checkpointLocation\", \"sliding_windows/checkpoint\") \\\n",
    "    .start()\n",
    ")\n",
    "\n",
    "ds_sales.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3f6ae-0f76-4e7c-8b97-31fd88b90032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sales.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamingproject",
   "language": "python",
   "name": "streamingproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
