{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2563081-c3a6-4992-b783-9163bb9f83b0",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Example\n",
    "\n",
    "Purpose: Reads a stream of messages from a Kafka topic and writes a stream of aggregations over sliding event-time window to memory.\n",
    "\n",
    "References: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "Author:  Gary A. Stafford\n",
    "\n",
    "Date: 2022-12-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b6543e-fc4c-4faf-9434-ee25cfcebd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    FloatType,\n",
    "    TimestampType,\n",
    "    BooleanType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322a21d0-4fb0-492e-b72d-327025bece2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_WATERMARK = \"10 seconds\"\n",
    "WINDOW_DURATION = \"20 seconds\"\n",
    "WINDOW_SLIDE = \"10 seconds\"\n",
    "PROCESSING_TIME = \"20 seconds\"\n",
    "SHOW_REFRESH = 20\n",
    "\n",
    "BOOTSTRAP_SERVERS = \"kafka:29092\"\n",
    "TOPIC_PURCHASES = \"demo.purchases\"\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": BOOTSTRAP_SERVERS,\n",
    "    \"subscribe\": TOPIC_PURCHASES,\n",
    "    \"startingOffsets\": \"latest\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0ecf312-4356-41ec-b816-c9f4702e7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.remote(\"sc://127.0.0.1:15002\").appName(\"kafka-streaming-query\").getOrCreate()\n",
    "df_sales = spark.readStream.format(\"kafka\").options(**KAFKA_OPTIONS).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f439c1a4-600a-41a3-aca4-cf125990c658",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[STREAMING_OUTPUT_MODE.UNSUPPORTED_DATASOURCE] Invalid streaming output mode: update. This output mode is not supported in Data Source json. SQLSTATE: 42KDE\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataSourceOutputModeUnsupportedError(QueryCompilationErrors.scala:1679)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:335)\n\tat org.apache.spark.sql.classic.DataStreamWriter.createV1Sink(DataStreamWriter.scala:335)\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:288)\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:3137)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2501)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m      1\u001b[39m schema = StructType(\n\u001b[32m      2\u001b[39m     [\n\u001b[32m      3\u001b[39m         StructField(\u001b[33m\"\u001b[39m\u001b[33mtransaction_time\u001b[39m\u001b[33m\"\u001b[39m, TimestampType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     ]\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m ds_sales = (\n\u001b[32m     16\u001b[39m     \u001b[43mdf_sales\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCAST(value AS STRING)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata.*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msales\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquantity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSC04\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msales\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquantity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransaction_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#.orderBy(F.col(\"product_id\").desc())\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax(transaction_time)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_number\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum(sales)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal_sales\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_number\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum(quantity)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal_items\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriteStream\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROCESSING_TIME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#.outputMode(\"complete\")\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrunning_totals/someoutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\work\\streaming-sales-generator\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\streaming\\readwriter.py:656\u001b[39m, in \u001b[36mDataStreamWriter.start\u001b[39m\u001b[34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\n\u001b[32m    648\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    649\u001b[39m     path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    654\u001b[39m     **options: \u001b[33m\"\u001b[39m\u001b[33mOptionalPrimitiveType\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    655\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mStreamingQuery\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_start_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputMode\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputMode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartitionBy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitionBy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqueryName\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqueryName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\work\\streaming-sales-generator\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\streaming\\readwriter.py:625\u001b[39m, in \u001b[36mDataStreamWriter._start_internal\u001b[39m\u001b[34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28mself\u001b[39m._write_proto.table_name = tableName\n\u001b[32m    624\u001b[39m cmd = \u001b[38;5;28mself\u001b[39m._write_stream.command(\u001b[38;5;28mself\u001b[39m._session.client)\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m (_, properties, _) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m start_result = cast(\n\u001b[32m    628\u001b[39m     pb2.WriteStreamOperationStartResult, properties[\u001b[33m\"\u001b[39m\u001b[33mwrite_stream_operation_start_result\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    629\u001b[39m )\n\u001b[32m    630\u001b[39m query = StreamingQuery(\n\u001b[32m    631\u001b[39m     session=\u001b[38;5;28mself\u001b[39m._session,\n\u001b[32m    632\u001b[39m     queryId=start_result.query_id.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    636\u001b[39m     name=start_result.name \u001b[38;5;28;01mif\u001b[39;00m start_result.name != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    637\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\work\\streaming-sales-generator\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1148\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations)\u001b[39m\n\u001b[32m   1146\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1147\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m data, _, metrics, observed_metrics, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m   1152\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\work\\streaming-sales-generator\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1560\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, self_destruct)\u001b[39m\n\u001b[32m   1557\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\work\\streaming-sales-generator\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1537\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\work\\streaming-sales-generator\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1811\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1809\u001b[39m     \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1810\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\work\\streaming-sales-generator\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1882\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   1879\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m info.metadata[\u001b[33m\"\u001b[39m\u001b[33merrorClass\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mINVALID_HANDLE.SESSION_CHANGED\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1880\u001b[39m                 \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1882\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   1883\u001b[39m                 info,\n\u001b[32m   1884\u001b[39m                 status.message,\n\u001b[32m   1885\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   1886\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   1887\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1889\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAnalysisException\u001b[39m: [STREAMING_OUTPUT_MODE.UNSUPPORTED_DATASOURCE] Invalid streaming output mode: update. This output mode is not supported in Data Source json. SQLSTATE: 42KDE\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataSourceOutputModeUnsupportedError(QueryCompilationErrors.scala:1679)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:335)\n\tat org.apache.spark.sql.classic.DataStreamWriter.createV1Sink(DataStreamWriter.scala:335)\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:288)\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteStreamOperationStart(SparkConnectPlanner.scala:3137)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2501)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"transaction_time\", TimestampType(), False),\n",
    "        StructField(\"transaction_id\", StringType(), False),\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"price\", FloatType(), False),\n",
    "        StructField(\"quantity\", IntegerType(), False),\n",
    "        StructField(\"is_member\", BooleanType(), True),\n",
    "        StructField(\"member_discount\", FloatType(), True),\n",
    "        StructField(\"add_supplements\", BooleanType(), True),\n",
    "        StructField(\"supplement_price\", FloatType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_sales = (\n",
    "    df_sales.selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(F.from_json(\"value\", schema=schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    "    .withColumn(\"sales\", F.col(\"price\")*F.col(\"quantity\"))\n",
    "    .where(F.col('product_id') == 'SC04')\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(F.sum(\"sales\"), F.sum(\"quantity\"), F.max(\"transaction_time\"))\n",
    "    .orderBy(F.col(\"product_id\").desc())\n",
    "    .select(\n",
    "        \"product_id\",\n",
    "        F.col('max(transaction_time)'),\n",
    "        F.format_number(\"sum(sales)\", 2).alias(\"total_sales\"),\n",
    "        F.format_number(\"sum(quantity)\", 0).alias(\"total_items\"),\n",
    "    )\n",
    "    .coalesce(1)\n",
    "    .writeStream\n",
    "    .trigger(processingTime=PROCESSING_TIME)\n",
    "    .outputMode(\"complete\")\n",
    "    #.outputMode(\"update\")\n",
    "    .format(\"json\")\n",
    "    .option(\"path\", \"running_totals/someoutput\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3f6ae-0f76-4e7c-8b97-31fd88b90032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sales.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesproject",
   "language": "python",
   "name": "salesproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
