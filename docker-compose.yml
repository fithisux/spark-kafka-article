# Command: docker stack deploy streaming-stack --compose-file docker/spark-kstreams-stack.yml
# Gary A. Stafford (2022-09-14)
# Updated: 2022-12-28

services:
  spark-master:
    image: "apache/spark:4.0.1-scala2.13-java17-python3-r-ubuntu"
    container_name: spark-master
    hostname: spark-master
    # environment:
    #   - SPARK_MODE=master
    #   - SPARK_RPC_AUTHENTICATION_ENABLED=no
    #   - SPARK_RPC_ENCRYPTION_ENABLED=no
    #   - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
    #   - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"
    volumes:
      - ./localvolumes/all_logs:/opt/spark/logs
      - ./localvolumes/all_conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf  
    command: 
      - "bash"
      - "-c"
      - "/opt/spark/sbin/start-master.sh --host spark-master && tail -f /dev/null"
    networks:
      - streaming-stack
  spark-worker:
    image: "apache/spark:4.0.1-scala2.13-java17-python3-r-ubuntu"
    container_name: spark-worker
    hostname: spark-worker
    # environment:
    #   - SPARK_MODE=worker
    #   - SPARK_MASTER_URL=spark://spark-master:7077
    #   - SPARK_WORKER_MEMORY=1G
    #   - SPARK_WORKER_CORES=1
    #   - SPARK_RPC_AUTHENTICATION_ENABLED=no
    #   - SPARK_RPC_ENCRYPTION_ENABLED=no
    #   - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
    #   - SPARK_SSL_ENABLED=no
    volumes:    
      - ./localvolumes/all_logs:/opt/spark/logs
      - ./localvolumes/all_conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf      
    command: 
      - "bash"
      - "-c"
      - "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 --host spark-worker  && tail -f /dev/null"      
    networks:
      - streaming-stack
  spark-connect:
    image: "apache/spark:4.0.1-scala2.13-java17-python3-r-ubuntu"
    container_name: spark-connect
    hostname: spark-connect
    ports:
      - "4040:4040"
      - "15002:15002"
    depends_on:
      - spark-master
    volumes:
      - ./localvolumes/jars/spark-connect_2.13-4.0.1.jar:/opt/spark/jars/spark-connect_2.13-4.0.1.jar
      - ./localvolumes/sc-work-dir:/opt/spark/work-dir:rw
      - ./localvolumes/ivycache:/opt/spark/ivycache:rw
      - ./localvolumes/all_logs:/opt/spark/logs
      - ./localvolumes/all_conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf      
    command: 
      - "bash"
      - "-c"
      - "/opt/spark/sbin/start-connect-server.sh --jars /opt/spark/jars/spark-connect_2.13-4.0.1.jar && tail -f /dev/null"
    networks:
    - streaming-stack
  kafka:
    image: docker.io/bitnami/kafka:latest
    ports:
      - "9092:9092"
    volumes:
      - "kafka_data:/bitnami"
    environment:
      # KRaft settings
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      # Listeners
      - KAFKA_CFG_LISTENERS=LISTENER_BOB://kafka:29092,LISTENER_FRED://kafka:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=LISTENER_BOB://kafka:29092,LISTENER_FRED://localhost:9092
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=LISTENER_BOB
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
    networks:
      - streaming-stack
  kafka-ui:
    image: "provectuslabs/kafka-ui"
    ports:
      - "9080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=streaming-demo
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
    depends_on:
      - kafka
    networks:
      - streaming-stack
  # jupyter:
  #   image: quay.io/jupyter/datascience-notebook:latest
  #   ports:
  #     - "8888:8888"
  #   volumes:
  #     - "jupyter_data:/home/jovyan/work"
  #   networks:
  #     - streaming-stack

volumes:
  kafka_data:
  jupyter_data:
  
networks:
  streaming-stack:
